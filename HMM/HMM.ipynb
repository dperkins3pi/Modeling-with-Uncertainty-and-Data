{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume 3: Discrete Hidden Markov Models\n",
    "    Daniel Perkins\n",
    "    MATH 407\n",
    "    1/20/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import codecs\n",
    "from hmmlearn.hmm import CategoricalHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems 1-5\n",
    "This is the HMM class that you will be adding functions to throughout the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"\n",
    "    Finite state space hidden Markov model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Problem 1\n",
    "    def __init__(self, A, B, pi):\n",
    "        \"\"\"\n",
    "        Initialize an HMM with parameters A, B, and pi.\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.pi = pi\n",
    "    \n",
    "    \n",
    "    # Problem 2\n",
    "    def forward_pass(self, z):\n",
    "        \"\"\"\n",
    "        Compute the forward probability matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : ndarray of shape (T,)\n",
    "            The observation sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        alpha : ndarray of shape (T, n)\n",
    "            The forward probability matrix\n",
    "        \"\"\"\n",
    "        # Initialize alpha as an empty matrix\n",
    "        T, n = np.shape(z)[0], np.shape(self.A)[0]\n",
    "        alpha = np.empty((T, n))\n",
    "        \n",
    "        # Compute alpha using array broadcasting (formula on page 268)\n",
    "        alpha[0] = self.pi * self.B[z[0]]\n",
    "        for t in range(1, T):\n",
    "            alpha[t] = self.B[z[t], :] * np.dot(alpha[t - 1], self.A.T)\n",
    "        \n",
    "        return alpha\n",
    "        \n",
    "    \n",
    "    # Problem 4\n",
    "    def backward_pass(self, z):\n",
    "        \"\"\"\n",
    "        Compute the backward probability matrix and gamma values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : ndarray of shape (T,)\n",
    "            The observation sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        beta : ndarray of shape (T, n)\n",
    "            The backward probability matrix\n",
    "        gamma : ndarray of shape (T, n)\n",
    "            The state probability matrix\n",
    "        \"\"\"\n",
    "        # Initialize the arrays\n",
    "        T, n = np.shape(z)[0], np.shape(self.A)[0]\n",
    "        beta, gamma = np.empty((T, n)), np.empty((T, n))\n",
    "        \n",
    "        # Use array broadcasting and matrix multiplication to compute beta\n",
    "        beta[T-1] = np.ones(n)\n",
    "        for t in range(T-2, -1, -1):\n",
    "            beta[t, :] = (self.A.T @ (beta[t+1, :] * self.B[z[t+1], :]))\n",
    "        \n",
    "        # Find gamma\n",
    "        alpha = self.forward_pass(z)\n",
    "        gamma = (alpha * beta) / np.sum(alpha[-1, :])\n",
    "        \n",
    "        return beta, gamma\n",
    "        \n",
    "    \n",
    "    # Problem 5\n",
    "    def viterbi_algorithm(self, z):\n",
    "        \"\"\"\n",
    "        Compute the most likely hidden state sequence using the Viterbi algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z : ndarray of shape (T,)\n",
    "            The observation sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x*: ndarray of shape (T,)\n",
    "            The most likely state sequence\n",
    "        \"\"\"\n",
    "        T, n = np.shape(z)[0], np.shape(self.A)[0]\n",
    "        \n",
    "        # Compute eta using array broadcasting\n",
    "        eta = np.empty((T, n))\n",
    "        eta[0] = self.B[z[0]] * self.pi                \n",
    "        for t in range(1, T):\n",
    "            eta[t, :] = np.max(self.B[z[t], :, None] * self.A * eta[t-1, None, :], axis=1)\n",
    "    \n",
    "        # Compute x using array broadcasting\n",
    "        x = np.empty_like(z)\n",
    "        x[T-1] = np.argmax(eta[T-1])            \n",
    "        for t in range(T-2, -1, -1):\n",
    "            x[t] = np.argmax(self.A[x[t+1], :] * eta[t, :])\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 test case\n",
    "\n",
    "Use the following HMM and code to test your HMM class.\n",
    "Compare the output to `forward_pass` with the lab pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009629599999999999\n"
     ]
    }
   ],
   "source": [
    "pi = np.array([.6, .4])\n",
    "A = np.array([[.7, .4], [.3, .6]])\n",
    "B = np.array([[.1, .7], [.4, .2], [.5, .1]])\n",
    "z_example = np.array([0, 1, 0, 2])\n",
    "example_hmm = HMM(A, B, pi)\n",
    "\n",
    "alpha = example_hmm.forward_pass(z_example)\n",
    "print(np.sum(alpha[-1, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Consider the following (very simplified) model of the price of a stock over time as an HMM.\n",
    "The observation states will be the change in the value of the stock.\n",
    "For simplicity, we will group these into five values: large decrease, small decrease, no change, small increase, large increase, labeled as integers from 0 to 4.\n",
    "The hidden state will be the overall trends of the market.\n",
    "We'll consider the market to have three possible states: declining in value (bear market), not changing in value (stagnant), and increasing in value (bull market), labeled as integers from 0 to 2.\n",
    "Let the HMM modeling this scenario have parameters\n",
    "$$\n",
    "\\boldsymbol\\pi=\\begin{bmatrix}\n",
    "1/3 \\\\ 1/3 \\\\ 1/3\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "A=\\begin{bmatrix}\n",
    "0.5 & 0.3 & 0 \\\\\n",
    "0.5 & 0.3 & 0.3 \\\\\n",
    "0 & 0.4 & 0.7\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B=\\begin{bmatrix}\n",
    "0.3 & 0.1 & 0 \\\\\n",
    "0.3 & 0.2 & 0.1 \\\\\n",
    "0.3 & 0.4 & 0.3 \\\\\n",
    "0.1 & 0.2 & 0.4 \\\\\n",
    "0 & 0.1 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The file `stocks.npy` contains a sequence of 50 observations drawn from this HMM.\n",
    "What is the probability of this observation sequence given these model parameters?\n",
    "Use your implementation of the forward pass algorithm from Problem 2 to find the answer.\n",
    "Note that the answer is very small, because there are lots of possible observation sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM parameter setup\n",
    "pi = np.array([1/3, 1/3, 1/3])\n",
    "A = np.array([\n",
    "    [0.5, 0.3, 0.0],\n",
    "    [0.5, 0.3, 0.3],\n",
    "    [0.0, 0.4, 0.7]\n",
    "])\n",
    "B = np.array([\n",
    "    [0.3, 0.1, 0.0],\n",
    "    [0.3, 0.2, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.0, 0.1, 0.2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of this observation, given these parameters is:\n",
      " 6.671115114537779e-34\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "data = np.load(\"stocks.npy\")\n",
    "\n",
    "# Create the HMM and find the probability of the data\n",
    "hmm = HMM(A, B, pi)\n",
    "alpha = hmm.forward_pass(data)\n",
    "probability = np.sum(alpha[-1, :])\n",
    "print(\"The probability of this observation, given these parameters is:\\n\", probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Create a method `backward_pass` in your HMM class to implement the backward pass algorithm.\n",
    "This function should accept the observation sequence $\\mathbf{z}$ and return two arrays of the $\\beta_t(i)$ and $\\gamma_t(i)$ values.\n",
    "\n",
    "Test your function on the example HMM, and compare the output with the lab pdf.\n",
    "\n",
    "With your function and the stock model from Problem 3, answer the following question: given the observation sequence in `stocks.npy`, what is the most likely initial hidden state $X_0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0302  0.02792]\n",
      " [0.0812  0.1244 ]\n",
      " [0.38    0.26   ]\n",
      " [1.      1.     ]]\n",
      "[[0.18816981 0.81183019]\n",
      " [0.51943175 0.48056825]\n",
      " [0.22887763 0.77112237]\n",
      " [0.8039794  0.1960206 ]]\n"
     ]
    }
   ],
   "source": [
    "pi = np.array([.6, .4])\n",
    "A = np.array([[.7, .4], [.3, .6]])\n",
    "B = np.array([[.1, .7], [.4, .2], [.5, .1]])\n",
    "z_example = np.array([0, 1, 0, 2])\n",
    "example_hmm = HMM(A, B, pi)\n",
    "\n",
    "# Test case; compare your output with what is in the lab pdf\n",
    "beta, gamma = example_hmm.backward_pass(z_example)\n",
    "print(beta)\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely initial hidden state X_0 is:\n",
      "State 1\n"
     ]
    }
   ],
   "source": [
    "# HMM parameter setup\n",
    "pi = np.array([1/3, 1/3, 1/3])\n",
    "A = np.array([\n",
    "    [0.5, 0.3, 0.0],\n",
    "    [0.5, 0.3, 0.3],\n",
    "    [0.0, 0.4, 0.7]\n",
    "])\n",
    "B = np.array([\n",
    "    [0.3, 0.1, 0.0],\n",
    "    [0.3, 0.2, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.0, 0.1, 0.2]\n",
    "])\n",
    "\n",
    "# Load in data\n",
    "data = np.load(\"stocks.npy\")\n",
    "\n",
    "# Create the HMM and find the probability of the data\n",
    "example_hmm = HMM(A, B, pi)\n",
    "beta, gamma = example_hmm.backward_pass(data)\n",
    "most_likely_initial_state = np.argmax(gamma[0])\n",
    "\n",
    "print(f\"The most likely initial hidden state X_0 is:\\nState {most_likely_initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "Creating a method `viterbi_algorithm` in your HMM class to implement the Viterbi algorithm.\n",
    "This function should accept the observation sequence $\\mathbf{z}$ and return the most likely state sequence $\\mathbf{x}^*$.\n",
    "\n",
    "Test your function on the example HMM and compare output with the lab pdf.\n",
    "\n",
    "Apply your function to the stock market HMM from Problem 3.\n",
    "With the observaition sequence from `stocks.npy`, what is the most likely sequence of hidden states?\n",
    "Is the initial state of the most likely sequence the same as the most likely initial state you found in Problem 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "pi = np.array([.6, .4])\n",
    "A = np.array([[.7, .4], [.3, .6]])\n",
    "B = np.array([[.1, .7], [.4, .2], [.5, .1]])\n",
    "z_example = np.array([0, 1, 0, 2])\n",
    "example_hmm = HMM(A, B, pi)\n",
    "\n",
    "# Test case\n",
    "xstar = example_hmm.viterbi_algorithm(z_example)\n",
    "print(xstar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely sequence of hidden states is:\n",
      "[0 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 0 0 0 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 1 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# HMM parameter setup\n",
    "pi = np.array([1/3, 1/3, 1/3])\n",
    "A = np.array([\n",
    "    [0.5, 0.3, 0.0],\n",
    "    [0.5, 0.3, 0.3],\n",
    "    [0.0, 0.4, 0.7]\n",
    "])\n",
    "B = np.array([\n",
    "    [0.3, 0.1, 0.0],\n",
    "    [0.3, 0.2, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.0, 0.1, 0.2]\n",
    "])\n",
    "\n",
    "# Load in data\n",
    "data = np.load(\"stocks.npy\")\n",
    "\n",
    "# Create the HMM and find the probability of the data\n",
    "hmm = HMM(A, B, pi)\n",
    "xstar = hmm.viterbi_algorithm(data)\n",
    "\n",
    "print(\"The most likely sequence of hidden states is:\")\n",
    "print(xstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely sequence of hidden states is given above. This time, the initial state is not the same as what we found in problem 4. This makes sense because the Viterbi algorithm relies on the initial state distribution, using more information than problem 4 (as it is measuring a different thing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "\n",
    "Train a `hmmlearn.hmm.CategoricalHMM` on `declaration.txt`. Use `N=2` states and `M=len(set(obs))=27` observation values (26 lower case characters and 1 whitespace character).\n",
    "Use `n_iter=200` and `tol=1e-4`.\n",
    "\n",
    "Once the learning algorithm converges, analyze the state observation matrix $B$. Note which rows correspond to the largest and smallest probability values in each column of $B$, and check the corresponding characters. The HMM should have detected a vowel state and a consonant state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_translate(a, my_dict):\n",
    "    # translate numpy array from symbols to state numbers or vice versa\n",
    "    return np.vectorize(my_dict.__getitem__)(a)\n",
    "\n",
    "def prep_data(filename):\n",
    "    \"\"\"\n",
    "    Reads in the file and prepares it for use in an HMM.\n",
    "    Returns:\n",
    "        symbols (dict): a dictionary that maps characters to their integer values\n",
    "        obs_sequence (ndarray): an array of integers representing the read-in text\n",
    "    \"\"\"\n",
    "    # Get the data as a single string\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        data = f.read().lower()  # and convert to all lower case\n",
    "    # remove punctuation and newlines\n",
    "    remove_punct_map = {ord(char): \n",
    "                        None for char in string.punctuation+\"\\n\\r\"}\n",
    "    data = data.translate(remove_punct_map)\n",
    "    # make a list of the symbols in the data\n",
    "    symbols = sorted(list(set(data)))\n",
    "    # convert the data to a NumPy array of symbols\n",
    "    a = np.array(list(data))\n",
    "    # make a conversion dictionary from symbols to state numbers\n",
    "    symbols_to_obsstates = {x: i for i, x in enumerate(symbols)}\n",
    "    # convert the symbols in a to state numbers\n",
    "    obs_sequence = vec_translate(a, symbols_to_obsstates)\n",
    "    return symbols, obs_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " , 0.0517, 0.2975\n",
      "a, 0.0000, 0.1321\n",
      "b, 0.0225, 0.0000\n",
      "c, 0.0437, 0.0000\n",
      "d, 0.0598, 0.0000\n",
      "e, 0.0000, 0.2379\n",
      "f, 0.0427, 0.0000\n",
      "g, 0.0308, 0.0001\n",
      "h, 0.0828, 0.0000\n",
      "i, 0.0000, 0.1243\n",
      "j, 0.0038, 0.0000\n",
      "k, 0.0030, 0.0004\n",
      "l, 0.0541, 0.0000\n",
      "m, 0.0342, 0.0000\n",
      "n, 0.1146, 0.0000\n",
      "o, 0.0037, 0.1378\n",
      "p, 0.0327, 0.0000\n",
      "q, 0.0014, 0.0000\n",
      "r, 0.1008, 0.0000\n",
      "s, 0.1134, 0.0000\n",
      "t, 0.1517, 0.0001\n",
      "u, 0.0000, 0.0579\n",
      "v, 0.0176, 0.0000\n",
      "w, 0.0230, 0.0000\n",
      "x, 0.0021, 0.0000\n",
      "y, 0.0090, 0.0119\n",
      "z, 0.0009, 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "symbols, obs = prep_data('declaration.txt')\n",
    "\n",
    "# Train the model\n",
    "h = CategoricalHMM(n_components=2, n_iter=200, tol=1e-4)\n",
    "h.fit(obs.reshape(-1, 1))\n",
    "\n",
    "pi = h.startprob_\n",
    "A = h.transmat_.T\n",
    "B = h.emissionprob_.T\n",
    "\n",
    "for i in range(len(B)):\n",
    "    print(u\"{}, {:0.4f}, {:0.4f}\".format(symbols[i], *B[i,:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears, that after a few runs, one of them results in a column where only the vowels have high probabilities (and the constants are practically at 0). This makes sense because vowels are the most common letters in the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "\n",
    "Repeat the same calculation with `WarAndPeace.txt` with 2 hidden states. Interpret/explain your results. Which Cyrillic characters appear to be vowels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " , 0.2146, 0.0877\n",
      "а, 0.0000, 0.1760\n",
      "б, 0.0250, 0.0000\n",
      "в, 0.0655, 0.0000\n",
      "г, 0.0296, 0.0000\n",
      "д, 0.0385, 0.0000\n",
      "е, 0.0180, 0.1427\n",
      "ж, 0.0140, 0.0000\n",
      "з, 0.0252, 0.0000\n",
      "и, 0.0017, 0.1315\n",
      "й, 0.0149, 0.0000\n",
      "к, 0.0497, 0.0010\n",
      "л, 0.0719, 0.0000\n",
      "м, 0.0381, 0.0000\n",
      "н, 0.0973, 0.0000\n",
      "о, 0.0000, 0.2407\n",
      "п, 0.0346, 0.0062\n",
      "р, 0.0597, 0.0000\n",
      "с, 0.0513, 0.0280\n",
      "т, 0.0780, 0.0000\n",
      "у, 0.0000, 0.0590\n",
      "ф, 0.0018, 0.0003\n",
      "х, 0.0111, 0.0000\n",
      "ц, 0.0049, 0.0000\n",
      "ч, 0.0167, 0.0038\n",
      "ш, 0.0109, 0.0000\n",
      "щ, 0.0047, 0.0000\n",
      "ъ, 0.0003, 0.0003\n",
      "ы, 0.0000, 0.0376\n",
      "ь, 0.0009, 0.0433\n",
      "э, 0.0000, 0.0066\n",
      "ю, 0.0079, 0.0024\n",
      "я, 0.0128, 0.0328\n",
      "ё, 0.0000, 0.0001\n"
     ]
    }
   ],
   "source": [
    "symbols, obs = prep_data('WarAndPeace.txt')\n",
    "\n",
    "# Train the model\n",
    "h = CategoricalHMM(n_components=2, n_iter=200, tol=1e-4)\n",
    "h.fit(obs.reshape(-1, 1))\n",
    "\n",
    "pi = h.startprob_\n",
    "A = h.transmat_.T\n",
    "B = h.emissionprob_.T\n",
    "\n",
    "for i in range(len(B)):\n",
    "    print(u\"{}, {:0.4f}, {:0.4f}\".format(symbols[i], *B[i,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the results of this model are conclusive, the letters most likely to be vowels are \"a\", \"e\", \"и\", \"o\", \"y\", \"ы\", \"ь\", and \"я\". This is because they have the highest probabilities in the column with a lot of zeroes. Of course, we cannot assume that these are the Russian vowels without more information. But, it does give us a good idea of what is more likely (or at least which letters are the most common in Russian)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".acme-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
